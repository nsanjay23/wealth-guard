import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import MinMaxScaler
import joblib
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Define the data directory relative to this script
DATA_DIR = os.path.join(os.path.dirname(__file__), 'data')

def process_data(file_name):
    file_path = os.path.join(DATA_DIR, file_name)
    
    # 1. Load Data (Using read_csv since your folder has .csv files)
    # Using 'thousands' parameter to handle commas automatically if possible, 
    # but we will do manual cleaning to be safe.
    df = pd.read_csv(file_path)
    
    # 2. Clean Date and Sort
    # The format appears to be Day-Month-Year based on your previous image
    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True) 
    df = df.sort_values('Date', ascending=True).reset_index(drop=True)
    
    # 3. Clean numeric columns (Remove 'M', 'K', '%', and commas)
    def clean_currency(x):
        if isinstance(x, str):
            x = x.replace(',', '')
            if 'M' in x: return float(x.replace('M', '')) * 1_000_000
            if 'K' in x: return float(x.replace('K', '')) * 1_000
            if '%' in x: return float(x.replace('%', ''))
        return float(x)

    # Columns to clean
    cols_to_clean = ['Price', 'Open', 'High', 'Low', 'Vol.']
    # Check if 'Change %' exists, add it if you need it, otherwise ignore
    
    for col in cols_to_clean:
        if col in df.columns:
            df[col] = df[col].apply(clean_currency)

    # Rename 'Price' to 'Close' for standard naming
    df.rename(columns={'Price': 'Close', 'Vol.': 'Volume'}, inplace=True)

    # 4. Calculate Technical Indicators
    
    # SMA (Simple Moving Average)
    df['SMA_20'] = df['Close'].rolling(window=20).mean()
    df['SMA_50'] = df['Close'].rolling(window=50).mean()

    # RSI (Relative Strength Index - 14 days)
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))

    # MACD (12, 26, 9)
    exp1 = df['Close'].ewm(span=12, adjust=False).mean()
    exp2 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp1 - exp2

    # Bollinger Bands (20, 2 std dev)
    df['BB_Middle'] = df['Close'].rolling(window=20).mean()
    df['BB_Upper'] = df['BB_Middle'] + 2*df['Close'].rolling(window=20).std()

    # Drop NaN values generated by rolling windows (first 50 rows)
    df.dropna(inplace=True)
    
    # Return only the columns we need for the model
    return df[['Date', 'Close', 'Volume', 'SMA_20', 'SMA_50', 'RSI', 'MACD', 'BB_Upper']]

def create_sequences(data, window_size=60):
    X, y = [], []
    # Loop through the data to create "windows"
    for i in range(len(data) - window_size):
        # Input: The 60 days of data (all features)
        X.append(data[i:(i + window_size)])
        
        # Output: The Close price of the NEXT day (Index 0 is Close price)
        y.append(data[i + window_size, 0]) 
        
    return np.array(X), np.array(y)

def build_lstm_model(input_shape):
    model = Sequential()
    
    # Layer 1: LSTM with Return Sequences (so it can feed into the next LSTM layer)
    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))
    model.add(Dropout(0.2)) # Randomly ignore 20% of neurons to prevent over-reliance
    
    # Layer 2: LSTM without Return Sequences (feeds into the Dense layer)
    model.add(LSTM(units=50, return_sequences=False))
    model.add(Dropout(0.2))
    
    # Output Layer: Predicts a single continuous value (Price)
    model.add(Dense(units=1)) 
    
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# --- PHASE 4: MAIN TRAINING LOOP ---
if __name__ == "__main__":
    # 1. Get list of files
    files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv')]
    
    print(f"Found {len(files)} companies to train. Starting process...\n")
    print("-" * 50)

    for file in files:
        try:
            # Extract Company Name (e.g., 'BPCL Historical Data.csv' -> 'BPCL')
            company_name = file.split(' ')[0]
            print(f"Training model for: {company_name}")

            # A. Process Data
            df = process_data(file)
            
            # B. Scale Data
            # We scale ALL features (Close, Volume, SMA, etc.)
            scaler = MinMaxScaler(feature_range=(0, 1))
            data_values = df.drop('Date', axis=1).values
            scaled_data = scaler.fit_transform(data_values)
            
            # SAVE SCALER (Critical for predictions later)
            scaler_filename = f"{company_name}_scaler.pkl"
            joblib.dump(scaler, scaler_filename)
            
            # C. Create Sequences
            WINDOW_SIZE = 60
            X, y = create_sequences(scaled_data, WINDOW_SIZE)
            
            # D. Build & Train Model
            # Input shape is (60 steps, 7 features)
            model = build_lstm_model((X.shape[1], X.shape[2]))
            
            # Training: 20 Epochs is a good balance for speed/accuracy
            # verbose=0 keeps the console clean, we just print start/finish
            model.fit(X, y, epochs=20, batch_size=32, verbose=0)
            
            # E. Save Model
            model_filename = f"{company_name}_model.h5"
            model.save(model_filename)
            
            print(f"✓ Saved: {model_filename} & {scaler_filename}")
            print("-" * 50)
            
        except Exception as e:
            print(f"❌ Error training {company_name}: {e}")

    print("\nAll models trained successfully!")
    print("You can now proceed to Phase 5 (The Prediction Script).")